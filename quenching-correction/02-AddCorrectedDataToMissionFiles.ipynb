{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for adding the final version of the corrected chlorophyll data to the binned, dark corrected, mission netCDF files. Rather than redo the correction for every mission, we're going to pull the relevant slice from the `quenchingCorrectionTests.nc` file, and add them to the appropriate mission nc file.\n",
    "\n",
    "# Final quenching correction method: FLZn\n",
    "\n",
    "The FLZn method performed the best of all the evaluated methods. The FLZn method has the following details:\n",
    "\n",
    "1. Extrapolates chlfl between the last profile from the previous night and the first profile of the following night to use as the reference (non-quenched) chl data\n",
    "2. Does not use a Zeu condition (hard limit or based on PAR)\n",
    "\n",
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import os \n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the mission nc files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parentDir = '../../../data/mission-netcdf/darkCountCorrected/'\n",
    "files = os.listdir(parentDir)\n",
    "gFiles = [parentDir + ff for ff in files if os.path.isfile(parentDir+ff)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the corrected chl data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../../../data/quenching-evaluation-netcdf/quenchingCorrectionTests.nc'\n",
    "cdata_all = xr.open_dataset(filepath)\n",
    "cdata = cdata_all.sel(chlcorrmethod='FLZnSC').reset_coords('chlcorrmethod',drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making mission id reference dictionary\n",
    "\n",
    "The glider missions are labeled differently within the `quenchingCorrectionTests` dataset, and in their filenames, so we need a reference dictionary to match the two together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullnames = [ff[:-3] for ff in files if os.path.isfile(parentDir+ff)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "id0 = [g[0] for g in fullnames]\n",
    "id1 = [re.search('[0-9]*\\.?[0-9]',gid).group() for gid in fullnames]\n",
    "ids = [i0 + i1 for i0,i1 in zip(id0,id1)]\n",
    "idref = {v: k for k, v in zip(ids,fullnames)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'henry-mission18': 'h18',\n",
       " 'grampus-mission1': 'g1',\n",
       " 'grampus-mission2.1': 'g2.1',\n",
       " 'grampus-mission2': 'g2',\n",
       " 'grampus-mission3': 'g3',\n",
       " 'grampus-mission4': 'g4',\n",
       " 'grampus-mission5': 'g5',\n",
       " 'grampus-mission6': 'g6',\n",
       " 'grampus-mission7': 'g7',\n",
       " 'grampus-mission8': 'g8',\n",
       " 'henry-mission1': 'h1',\n",
       " 'henry-mission2': 'h2',\n",
       " 'henry-mission3': 'h3',\n",
       " 'henry-mission4': 'h4',\n",
       " 'henry-mission5': 'h5',\n",
       " 'henry-mission6': 'h6',\n",
       " 'henry-mission7.1': 'h7.1',\n",
       " 'henry-mission7': 'h7',\n",
       " 'henry-mission8': 'h8',\n",
       " 'henry-mission9': 'h9',\n",
       " 'henry-mission10': 'h10',\n",
       " 'henry-mission11': 'h11',\n",
       " 'henry-mission12': 'h12',\n",
       " 'henry-mission13': 'h13',\n",
       " 'henry-mission14': 'h14',\n",
       " 'henry-mission15': 'h15',\n",
       " 'henry-mission16': 'h16',\n",
       " 'henry-mission17': 'h17'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding corrected data to mission files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsaveparentdir = '../../../data/mission-netcdf/quenchingCorrected/'\n",
    "\n",
    "for gf in gFiles:\n",
    "    gdata = xr.open_dataset(gf)\n",
    "    \n",
    "    midlong = re.split('/',gf)[-1]\n",
    "    mid = idref[midlong[:-3]]\n",
    "    \n",
    "    for dd in ['east','west']:\n",
    "        try:\n",
    "            chlcorr = cdata[mid+'_'+dd]\n",
    "            gdata['chlflCorr_'+dd] = chlcorr\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    gdata.to_netcdf(newsaveparentdir+midlong)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
